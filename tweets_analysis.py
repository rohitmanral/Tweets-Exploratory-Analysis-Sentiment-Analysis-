# -*- coding: utf-8 -*-
"""Tweets_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MR-ClI0_gCtcWzN52jkhsqSeOyhdNBww

# **ADS (Task- 2)**

# installing tweepy library
"""

pip install tweepy

# import the module 
import tweepy 
  
# assign the values accordingly 
consumer_key = 'yVG01ZZvuAZ0IKFan6nUuXXXX'
consumer_secret = 'T0fSLrnq1d2kmIFJcq3zx99Xu5GQg4Mz8k07pDrkSe3LmpXXXX'
access_token = '1239936625263529984-R8dX0F7gk89c9bUk09BNuiVj6TXXXX'
access_token_secret = 'DsLVMo1ZNPGhHKCZtXNh4cJ8ZMYelUl7PfOYQzc1UXXXX'

# authorization of consumer key and consumer secret 
auth = tweepy.OAuthHandler(consumer_key, consumer_secret) 
  
# set access to user's access key and access secret  
auth.set_access_token(access_token, access_token_secret) 
  
# calling the api  
api = tweepy.API(auth)

"""# XXXXXXXXXXXXXXXXXXXXXXXXXX

# **Data Preprocessing**

# **Getting Tweets of `user 1` (realDonaldTrump)**
"""

tweets_user1 = api.user_timeline(screen_name = "realDonaldTrump",
tweet_mode = 'extended',
count = 200)

tweets_user1

"""# Writing Tweets to a **JSON** File"""

import json
list_of_dicts = []
for json_tweet in tweets_user1:
  list_of_dicts.append(json_tweet._json)
with open('tweet_user1.json', mode = 'w') as file:
  file.write(json.dumps(list_of_dicts, indent = 4))

"""# Reading Tweets from a File"""

list = []
with open('tweet_user1.json', encoding='utf-8', mode = 'r') as file:
  list_of_dicts = json.load(file)
  for dict in list_of_dicts:
    list.append({'tweet_id': str(dict['id']),
'full_text': str(dict['full_text']),

'pictures': dict['entities'],

'hashtags': dict['entities']['hashtags'],

'posting_time': dict['created_at'],
})

"""# Generating a DataFrame using Tweet Info"""

import pandas as pd
df_user1 = pd.DataFrame(list, columns = ['tweet_id',
'full_text',
'hashtags',
'pictures',
'posting_time'])

df_user1.head()

import numpy as np

tag = [] 
for i in df_user1["hashtags"]:
  try:
    tag.append(i[0]['text']) 
  except:
    tag.append(np.nan) 
    continue 

df_user1["hashtags"] = tag

df_user1.head()

"""## pic checking"""

pic = [] 
for i in df_user1["pictures"]:
  try:
    pic.append(i['media'][0]['media_url']) 
  except:
    pic.append(np.nan) 
    continue 

df_user1["pictures"] = pic

df_user1.head()

"""## link checking"""

pip install urlextract

from urlextract import URLExtract

extractor = URLExtract()

link = [] 
for i in df_user1["full_text"]:
  link.append(extractor.find_urls(i))

df_user1['links']=link

df_user1.links = df_user1.links.apply(lambda y: np.nan if len(y)==0 else y)

df_user1.head()

import numpy as np
import pandas as pd

df_user1['pictures'] = np.where(pd.notnull(df_user1['pictures']), 1, 0)

df_user1['links'] = np.where(pd.notnull(df_user1['links']), 1, 0)

df_user1['pic_or_link'] = np.where((df_user1['pictures'] == 0) & (df_user1['links'] == 0), 0, 1)

df_user1

from datetime import datetime


date = [] 
for i in df_user1["posting_time"]:
  date.append(datetime.strftime(datetime.strptime(i,'%a %b %d %H:%M:%S +0000 %Y'), '%d-%m-%Y %H:%M:%S'))
df_user1['posting_time']=date

df_user1.head()

date = [] 
for i in df_user1["posting_time"]:
  date.append(datetime.strftime(datetime.strptime(i,'%d-%m-%Y %H:%M:%S'), '%d-%m-%Y'))
df_user1['date']=date

df_user1.head()

date = [] 
for i in df_user1["date"]:
  date.append(datetime.strftime(datetime.strptime(i,'%d-%m-%Y'), '%m'))
df_user1['month-only']=date

df_user1.head()

time = [] 
for i in df_user1["posting_time"]:
  time.append(datetime.strftime(datetime.strptime(i,'%d-%m-%Y %H:%M:%S'), '%H:%M:%S'))
df_user1['time']=time

time = [] 
for i in df_user1["time"]:
  time.append(datetime.strftime(datetime.strptime(i,'%H:%M:%S'), '%H'))
df_user1['hour_only']=time

df_user1.head()

"""# Cleaning up Tweets: Removing @Handle"""

import re
import numpy as np
def remove_pattern(input_txt, pattern):
  r = re.findall(pattern, input_txt)
  for i in r:
    input_txt = re.sub(i, '', input_txt)
  return input_txt
# vectorize() evaluates pyfunc over successive tuples of the
# input arrays similar to the python map function, except it uses
# the broadcasting rules of numpy.
df_user1['full_text'] = np.vectorize(remove_pattern)(df_user1['full_text'], "@[\w]*")
print(df_user1['full_text'])

"""# Cleaning up Tweets"""

import nltk

nltk.download('stopwords')

from nltk.stem.porter import PorterStemmer
from nltk.corpus import stopwords
max_tweets = 199
corpus = []

for i in range(0, max_tweets):
  tweet = re.sub(r'https?:\/\/\S*', '',
  df_user1['full_text'][i], flags = re.MULTILINE)
  tweet = re.sub('[^a-zA-Z0-9]', ' ', tweet)
  tweet = tweet.lower()
  tweet = re.sub('rt', '', tweet)
  tweet = tweet.split()
  ps = PorterStemmer()
  tweet = [ps.stem(word) for word in tweet
           if not word in set(stopwords.words('english'))]
  if tweet == []: continue
  else:
    tweet = ' '.join(tweet)
    corpus.append(tweet)

df_user1['full_text'] = df_user1['full_text'].replace(r'http\S+', '', regex=True).replace(r'www\S+', '', regex=True)

df_user1['full_text'] = df_user1['full_text'].replace(r'RT\s:\s', '', regex=True)

df_user1['full_text'] = df_user1['full_text'].replace(r',', '', regex=True).replace(r':', '', regex=True).replace(r';', '', regex=True).replace(r'"', '', regex=True).replace(r'&', '', regex=True)

df_user1.head()

"""# XXXXXXXXXXXXXXXXXXXXXXXXXX

# **Getting Tweets of `user 2` (ScottMorrisonMP)**
"""

tweets_user2 = api.user_timeline(screen_name = "ScottMorrisonMP",
tweet_mode = 'extended',
count = 200)

tweets_user2

"""# Writing Tweets to a **JSON** File"""

import json
list_of_dicts = []
for json_tweet in tweets_user2:
  list_of_dicts.append(json_tweet._json)
with open('tweet_user2.json', mode = 'w') as file:
  file.write(json.dumps(list_of_dicts, indent = 4))

"""# Reading Tweets from a File"""

list = []
with open('tweet_user2.json', encoding='utf-8', mode = 'r') as file:
  list_of_dicts = json.load(file)
  for dict in list_of_dicts:
    list.append({'tweet_id': str(dict['id']),
'full_text': str(dict['full_text']),

'pictures': dict['entities'],

'hashtags': dict['entities']['hashtags'],

'posting_time': dict['created_at'],
})

"""# Generating a DataFrame using Tweet Info"""

import pandas as pd
df_user2 = pd.DataFrame(list, columns = ['tweet_id',
'full_text',
'hashtags',
'pictures',
'posting_time'])

df_user2.head()

tag = [] 
for i in df_user2["hashtags"]:
  try:
    tag.append(i[0]['text']) 
  except:
    tag.append(np.nan) 
    continue 

df_user2["hashtags"] = tag

df_user2.head()

"""## pic checking"""

pic = [] 
for i in df_user2["pictures"]:
  try:
    pic.append(i['media'][0]['media_url']) 
  except:
    pic.append(np.nan) 
    continue 

df_user2["pictures"] = pic

df_user2.head()

"""## link checking"""

link = [] 
for i in df_user2["full_text"]:
  link.append(extractor.find_urls(i))

df_user2['links']=link

df_user2.links = df_user2.links.apply(lambda y: np.nan if len(y)==0 else y)

df_user2.head()

import numpy as np
import pandas as pd

df_user2['pictures'] = np.where(pd.notnull(df_user2['pictures']), 1, 0)

df_user2['links'] = np.where(pd.notnull(df_user2['links']), 1, 0)

df_user2['pic_or_link'] = np.where((df_user2['pictures'] == 0) & (df_user2['links'] == 0), 0, 1)

df_user2.head()

from datetime import datetime


date = [] 
for i in df_user2["posting_time"]:
  date.append(datetime.strftime(datetime.strptime(i,'%a %b %d %H:%M:%S +0000 %Y'), '%d-%m-%Y %H:%M:%S'))
df_user2['posting_time']=date

df_user2.head()

date = [] 
for i in df_user2["posting_time"]:
  date.append(datetime.strftime(datetime.strptime(i,'%d-%m-%Y %H:%M:%S'), '%d-%m-%Y'))
df_user2['date']=date

df_user2.head()

date = [] 
for i in df_user2["date"]:
  date.append(datetime.strftime(datetime.strptime(i,'%d-%m-%Y'), '%m'))
df_user2['month-only']=date

df_user2.head()

time = [] 
for i in df_user2["posting_time"]:
  time.append(datetime.strftime(datetime.strptime(i,'%d-%m-%Y %H:%M:%S'), '%H:%M:%S'))
df_user2['time']=time

time = [] 
for i in df_user2["time"]:
  time.append(datetime.strftime(datetime.strptime(i,'%H:%M:%S'), '%H'))
df_user2['hour_only']=time

df_user2.head()

"""# Cleaning up Tweets: Removing @Handle"""

df_user2['full_text'] = np.vectorize(remove_pattern)(df_user2['full_text'], "@[\w]*")
print(df_user2['full_text'])

"""# Cleaning up Tweets"""

for i in range(0, max_tweets):
  tweet = re.sub(r'https?:\/\/\S*', '',
  df_user2['full_text'][i], flags = re.MULTILINE)
  tweet = re.sub('[^a-zA-Z0-9]', ' ', tweet)
  tweet = tweet.lower()
  tweet = re.sub('rt', '', tweet)
  tweet = tweet.split()
  ps = PorterStemmer()
  tweet = [ps.stem(word) for word in tweet
           if not word in set(stopwords.words('english'))]
  if tweet == []: continue
  else:
    tweet = ' '.join(tweet)
    corpus.append(tweet)

df_user2['full_text'] = df_user2['full_text'].replace(r'http\S+', '', regex=True).replace(r'www\S+', '', regex=True)

df_user2['full_text'] = df_user2['full_text'].replace(r'RT\s:\s', '', regex=True)

df_user2['full_text'] = df_user2['full_text'].replace(r',', '', regex=True).replace(r':', '', regex=True).replace(r';', '', regex=True).replace(r'"', '', regex=True).replace(r'&', '', regex=True)

df_user2.head()

"""# XXXXXXXXXXXXXXXXXXXXXXXXXX

# Writing both DataFrames to a **CSV** File
"""

df=pd.concat([df_user1,df_user2])

# Writing the DataFrame to a csv file.
df.to_csv('tweet_data.csv')

"""# XXXXXXXXXXXXXXXXXXXXXXXXXX

# **1) Pandas to plot the posting times of the tweets for the two users; the aim of the plot is to distinguish the two users.**

## Plotting posting times of user 1 (Donald Trump)
"""

dt=df_user1['posting_time'].value_counts()
print(dt)
dt.plot.bar(x=df_user1.index, rot=70, figsize=(40,8))

"""## Bar plot for exact date"""

d=df_user1['date'].value_counts()


print(d)
d.plot.bar(x=df_user1.index, rot=70, figsize=(35,8))

"""## Line plot for exact tweet hour"""

h=df_user1['hour_only'].value_counts()
print(h)
h.plot.line(x=df_user1.index, rot=70, figsize=(35,8))

"""# XXXXXXXXXXXXXXXX

## Plotting posting times of user 2 (Scott Morrison)
"""

dt=df_user2['posting_time'].value_counts()
print(dt)
dt.plot.bar(x=df_user2.index, rot=70, figsize=(40,8))

"""## Bar plot for exact date"""

d=df_user2['date'].value_counts()


print(d)
d.plot.bar(x=df_user2.index, rot=70, figsize=(35,8))

"""## Line plot for exact tweet hour"""

h=df_user2['hour_only'].value_counts()
print(h)
h.plot.line(x=df_user2.index, rot=70, figsize=(35,8))

"""# **Conclusion of 1): From the bar plots of two users, we got to know that:**  
`user 1` posted 4 tweets on 06-10-2020 18:48:47, 3 tweets on 09-10-2020 01:30:16 , and 2 tweets on 08-10-2020 19:31:02. However, all other tweets have different posting times.

`user 1` posted more than half of the tweets on 07-10-2020.

As per the Australian timing, `user 1` posted highest number of posts i.e. 34 at midnight, besides `user 1` posted 88 tweets out of 200 between midnight and 3 am.

`user 2` posted 3 tweets on 28-08-2020 08:51:23, then 2 tweets on each of the 10 different days. However, all other tweets have different posting times.

`user 2` posted 17 the tweets on 06-10-2020.

`user 2` posted 34% of the tweets between 8pm & 10pm. Besides, approximately 25% of the tweets posted in the afternoon and approximately 40 out of 200 of tweets in the evening.

# **2) Pandas to construct a bar chart of the proportions of tweets for each of the two users that contain pictures or links.**

## Plotting proportions of tweets for user 1 (Donald Trump) that contain pictures or links.
"""

p_or_l=df_user1.pic_or_link.value_counts() 
print(p_or_l)
p_or_l.plot.bar(x=df_user1.index, rot=0)

"""## Plotting proportions of tweets for user 2 (Scott Morrison) that contain pictures or links."""

p_or_l=df_user2.pic_or_link.value_counts() 
print(p_or_l)
p_or_l.plot.bar(x=df_user2.index, rot=0)

"""# **Conclusion of 2): From plotting proportions of two users that contain pictures or links, we got to know that:**  
48% tweets of `user 1` contain pictures or links, while 52% tweets of `user 1` does not contain pictures or links.

63.5% tweets of `user 2` contain pictures or links, while 36.5% tweets of `user 2` does not contain pictures or links.

# **3) Pandas to construct a histogram of the number of hashtags in tweets for each of the two users.**

## Plotting number of hashtags in tweets for user 1 (Donald Trump).
"""

df_user1['hashtags'].fillna(value=0,inplace=True)

h=df_user1.hashtags.value_counts() 
print(h)
h.plot.bar(x=df_user1.index, rot=60, figsize=(12,5))

h.hist(figsize=(25,8), bins = 50)

"""## Plotting number of hashtags in tweets for user 2 (Scott Morrison)."""

df_user2['hashtags'].fillna(value=0,inplace=True)

h=df_user2.hashtags.value_counts() 
print(h)
h.plot.bar(x=df_user2.index, rot=60, figsize=(20,6))

h.hist(figsize=(25,8), bins = 50)

"""# **Conclusion of 3): From plotting number of hashtags in tweets for two users, we got to know that:**  
91% tweets of `user 1` do not have any hashtag, 2% have `#Obamagate`. However, 7% tweets of `user 1` got totally different hashtags.

57.5% tweets of `user 2` do not have any hashtag, 9.5% have `#COVID19`, 6.5% have `#Budget2020`, 6% have `#LIVE `, 4.5% have `#JobKeeper`, & 4% have `#coronavirus`. However, other tweets of `user 2` got totally different hashtags.

# **4) Calculating the log odds ratio for each word used in the set of tweets, and listing the 20 words most strongly associated with each of the two users.**

* **Calculating the log odds ratio for each word used in the set of tweets**

Counting the Most Frequent Words for `user 1`
"""

sentence1 = df_user1['full_text'].tolist()

wordlist1=[]

for sentence in sentence1:
  wordlist1.append(sentence.split())

print(wordlist1)

words1 = []
while wordlist1:
  words1.extend(wordlist1.pop(0))

print(words1)

words1_new=[]

for i in words1:
  words1_new.append(i.lower())

print(words1_new)

len(words1_new)



"""Counting the Most Frequent Words for `user 2`"""

sentence2 = df_user2['full_text'].tolist()

wordlist2=[]

for sentence in sentence2:
  wordlist2.append(sentence.split())

print(wordlist2)

words2 = []
while wordlist2:
  words2.extend(wordlist2.pop(0))

print(words2)

words2_new=[]

for i in words2:
  words2_new.append(i.lower())

print(words2_new)

len(words2_new)

"""## Removing Stop Words"""

stopwords = ['a', 'about', 'above', 'across', 'after', 'afterwards']
stopwords += ['again', 'against', 'all', 'almost', 'alone', 'along']
stopwords += ['already', 'also', 'although', 'always', 'am', 'among']
stopwords += ['amongst', 'amoungst', 'amount', 'an', 'and', 'another']
stopwords += ['any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere']
stopwords += ['are', 'around', 'as', 'at', 'back', 'be', 'became']
stopwords += ['because', 'become', 'becomes', 'becoming', 'been']
stopwords += ['before', 'beforehand', 'behind', 'being', 'below']
stopwords += ['beside', 'besides', 'between', 'beyond', 'bill', 'both']
stopwords += ['bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant']
stopwords += ['co', 'computer', 'con', 'could', 'couldnt', 'cry', 'de']
stopwords += ['describe', 'detail', 'did', 'do', 'done', 'down', 'due']
stopwords += ['during', 'each', 'eg', 'eight', 'either', 'eleven', 'else']
stopwords += ['elsewhere', 'empty', 'enough', 'etc', 'even', 'ever']
stopwords += ['every', 'everyone', 'everything', 'everywhere', 'except']
stopwords += ['few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first']
stopwords += ['five', 'for', 'former', 'formerly', 'forty', 'found']
stopwords += ['four', 'from', 'front', 'full', 'further', 'get', 'give']
stopwords += ['go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her']
stopwords += ['here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers']
stopwords += ['herself', 'him', 'himself', 'his', 'how', 'however']
stopwords += ['hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed']
stopwords += ['interest', 'into', 'is', 'it', 'its', 'itself', 'keep']
stopwords += ['last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made']
stopwords += ['many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine']
stopwords += ['more', 'moreover', 'most', 'mostly', 'move', 'much']
stopwords += ['must', 'my', 'myself', 'name', 'namely', 'neither', 'never']
stopwords += ['nevertheless', 'next', 'nine', 'no', 'nobody', 'none']
stopwords += ['noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of']
stopwords += ['off', 'often', 'on','once', 'one', 'only', 'onto', 'or']
stopwords += ['other', 'others', 'otherwise', 'our', 'ours', 'ourselves']
stopwords += ['out', 'over', 'own', 'part', 'per', 'perhaps', 'please']
stopwords += ['put', 'rather', 're', 's', 'same', 'see', 'seem', 'seemed']
stopwords += ['seeming', 'seems', 'serious', 'several', 'she', 'should']
stopwords += ['show', 'side', 'since', 'sincere', 'six', 'sixty', 'so']
stopwords += ['some', 'somehow', 'someone', 'something', 'sometime']
stopwords += ['sometimes', 'somewhere', 'still', 'such', 'system', 'take']
stopwords += ['ten', 'than', 'that', 'the', 'their', 'them', 'themselves']
stopwords += ['then', 'thence', 'there', 'thereafter', 'thereby']
stopwords += ['therefore', 'therein', 'thereupon', 'these', 'they']
stopwords += ['thick', 'thin', 'third', 'this', 'those', 'though', 'three']
stopwords += ['three', 'through', 'throughout', 'thru', 'thus', 'to']
stopwords += ['together', 'too', 'top', 'toward', 'towards', 'twelve']
stopwords += ['twenty', 'two', 'un', 'under', 'until', 'up', 'upon']
stopwords += ['us', 'very', 'via', 'was', 'we', 'well', 'were', 'what']
stopwords += ['whatever', 'when', 'whence', 'whenever', 'where']
stopwords += ['whereafter', 'whereas', 'whereby', 'wherein', 'whereupon']
stopwords += ['wherever', 'whether', 'which', 'while', 'whither', 'who']
stopwords += ['whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with']
stopwords += ['within', 'without', 'would', 'yet', 'you', 'your']
stopwords += ['yours', 'yourself', 'yourselves']

"""Now getting rid of the stop words in a list is as easy as using another list comprehension. """

def removeStopwords(wordlist, stopwords):
  return [w for w in wordlist if w not in stopwords]

words1_new=removeStopwords(words1_new, stopwords)
print(words1_new)
total_user1=len(words1_new)
total_user1

words2_new=removeStopwords(words2_new, stopwords)
print(words2_new)
total_user2=len(words2_new)
total_user2

"""## **Common words for both the users**"""

s = set(words2_new)
list3 = [x for x in words1_new if x in s]
list3

words1_n=[]

words1_n = [e for e in words1_new if e in list3]
words1_n

words2_n=[]

words2_n = [e for e in words2_new if e in list3]
words2_n



"""## Common words series of both the users"""

words1=pd.Series(words1_n).value_counts()
words1

words2=pd.Series(words2_n).value_counts()
words2

"""# **Calculating the log odds ratio**"""

import math 

new_words=[]
new_log_rat=[]



for i in list3:
  new_words.append(i)
  X=((words1[i]+1)/(total_user1+1))/((words2[i]+1)/(total_user2+1))
  x=math.log2(X)
  new_log_rat.append(x)

print(new_words)
print(new_log_rat)

df = pd.DataFrame({'new_log_rat': new_log_rat}, index=new_words)

df.head()

"""## As these are soo many words, so I will fetch only top 20 words which are common for both the users"""

df.sort_values('new_log_rat')

df_user_2=df.head(10) #Scott Morrison
df_user_1=df.tail(10) #Donald Trump

df_user_2.head() #top 10 words joined with Scott Morrison

df_user_1.head() #top 10 words joined with Donald Trump

"""## **Our final 20 words associated with both the users**"""

df = pd.concat([df_user_1,df_user_2],ignore_index=False)
df

"""# **Therefore, these words most likely to be from `user 1` and most likely from `user 2`**"""

df.plot.barh(y='new_log_rat', figsize=(20,15))

"""* **Listing the 20 words most strongly associated with each of the two users**"""

df.index

print("The 20 words most strongly associated with each of the two users are {}".format(df.index))

"""# **5) Using nltk vader module to calculate the sentiment of each tweet, and then for each of the two users, calculate the average 'compound' sentiment for all their tweets.**"""

pip install vaderSentiment

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

"""## **Sentiment of each tweet of user 1 (Donald Trump).**"""

sentence1 = df_user1['full_text'].tolist()

analyzer = SentimentIntensityAnalyzer()
for sentence in sentence1:
    vs = analyzer.polarity_scores(sentence)
    print("{:-<65} {}".format(sentence, str(vs)))

"""# Bar plot for all the average scores of `user 1`"""

Sum_neg=0
Sum_neu=0
Sum_pos=0
Sum_cmpd=0

for sentence in sentence1:
    vs_neg = analyzer.polarity_scores(sentence)['neg']
    Sum_neg=Sum_neg+vs_neg

    vs_neu = analyzer.polarity_scores(sentence)['neu']
    Sum_neu=Sum_neu+vs_neu

    vs_pos = analyzer.polarity_scores(sentence)['pos']
    Sum_pos=Sum_pos+vs_pos

    vs_cmpd = analyzer.polarity_scores(sentence)['compound']
    Sum_cmpd=Sum_cmpd+vs_cmpd






avg_neg=round(Sum_neg/200,3)
avg_neu=round(Sum_neu/200,3)
avg_pos=round(Sum_pos/200,3)
avg_cmpd=round(Sum_cmpd/200,3)

print(avg_neg)
print(avg_neu)
print(avg_pos)
print(avg_cmpd)

import matplotlib.pyplot as plt
values = [avg_neg, avg_neu, avg_pos, avg_cmpd]
x = ('negative', 'neutral', 'positive','compound')
# Create a barplot.
bar_list = plt.bar(x, values)
# Set colours.
bar_list[0].set_color('pink')
bar_list[1].set_color('sienna')
bar_list[2].set_color('cornflowerblue')
bar_list[3].set_color('blue')
# Show graphic.
plt.show()

"""### **Therefore most of the `user 1` tweets have neutral sentiment**

## **Average 'compound' sentiment for all tweets of user 1 (Donald Trump).**
"""

Sum=0

for sentence in sentence1:
    vs = analyzer.polarity_scores(sentence)['compound']
    Sum=Sum+vs
avg_cmpd=round(Sum/200,3)
print("The average 'compound' sentiment for all tweets of user 1 (Donald Trump) is {}".format(avg_cmpd)    )

"""# XXXXXXXXXXXXXXXX

## **Sentiment of each tweet of user 2 (Scott Morrison).**
"""

sentence2 = df_user2['full_text'].tolist()

analyzer = SentimentIntensityAnalyzer()
for sentence in sentence2:
    vs = analyzer.polarity_scores(sentence)
    print("{:-<65} {}".format(sentence, str(vs)))

"""# Bar plot for all the average scores of `user 2`"""

Sum_neg=0
Sum_neu=0
Sum_pos=0
Sum_cmpd=0

for sentence in sentence2:
    vs_neg = analyzer.polarity_scores(sentence)['neg']
    Sum_neg=Sum_neg+vs_neg

    vs_neu = analyzer.polarity_scores(sentence)['neu']
    Sum_neu=Sum_neu+vs_neu

    vs_pos = analyzer.polarity_scores(sentence)['pos']
    Sum_pos=Sum_pos+vs_pos

    vs_cmpd = analyzer.polarity_scores(sentence)['compound']
    Sum_cmpd=Sum_cmpd+vs_cmpd






avg_neg=round(Sum_neg/200,3)
avg_neu=round(Sum_neu/200,3)
avg_pos=round(Sum_pos/200,3)
avg_cmpd=round(Sum_cmpd/200,3)

print(avg_neg)
print(avg_neu)
print(avg_pos)
print(avg_cmpd)

import matplotlib.pyplot as plt
values = [avg_neg, avg_neu, avg_pos, avg_cmpd]
x = ('negative', 'neutral', 'positive','compound')
# Create a barplot.
bar_list = plt.bar(x, values)
# Set colours.
bar_list[0].set_color('pink')
bar_list[1].set_color('sienna')
bar_list[2].set_color('cornflowerblue')
bar_list[3].set_color('blue')
# Show graphic.
plt.show()

"""### **Therefore most of the `user 1` tweets have neutral sentiment followed by compound ones.**

## **Average 'compound' sentiment for all tweets of user 2 (Scott Morrison).**
"""

Sum=0

for sentence in sentence2:
    vs = analyzer.polarity_scores(sentence)['compound']
    Sum=Sum+vs
avg_cmpd=round(Sum/200,3)
print("The average 'compound' sentiment for all tweets of user 2 (Scott Morrison) is {}".format(avg_cmpd)    )

"""## About the Scoring

* The compound score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. Calling it a 'normalized, weighted composite score' is accurate.

    It is also useful for researchers who would like to set standardized thresholds for classifying sentences as either positive, neutral, or negative. Typical threshold values (used in the literature cited on this page) are:

    1. positive sentiment: compound score >= 0.05
    2. neutral sentiment: (compound score > -0.05) and (compound score < 0.05)
    3. negative sentiment: compound score <= -0.05


* The pos, neu, and neg scores are ratios for proportions of text that fall in each category (so these should all add up to be 1... or close to it with float operation). These are the most useful metrics if you want multidimensional measures of sentiment for a given sentence.
"""